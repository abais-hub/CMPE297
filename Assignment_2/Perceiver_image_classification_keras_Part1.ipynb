{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceiver_image_classification_keras_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxHoonSp9jpk"
      },
      "source": [
        "# Image classification with Perceiver on cifar-10 dataset  \n",
        "1. Instead of the cifar-100, the cifar-10 dataset is used \n",
        "2. To cut traning time from ~2hours \n",
        "   - Image classification problem has been reduced to 2 classses  \n",
        "   - 5 epoch training is performed to cut runtime \n",
        "\n",
        "[Reference](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzimpXZl9jpp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VDiKHh7fKfP",
        "outputId": "d1e5b2c4-605a-42ff-ca78-d97e690d2bb5"
      },
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjpMCSNE9jpq"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwkVxXmv9jpr"
      },
      "source": [
        "# Read in the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDgRz-Sk9jpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223b4718-0a4b-4b8b-f141-af8cb8ea8601"
      },
      "source": [
        "num_classes = 2\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(xtrain, ytrain), (xtest, ytest) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Reduce the number of images further to cut training time from ~2hours\n",
        "x_train = []\n",
        "y_train = []\n",
        "for i, y in enumerate(ytrain):\n",
        "  if y < 2:\n",
        "    x_train.append(xtrain[i])\n",
        "    y_train.append(y)\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "for i, y in enumerate(ytest):\n",
        "  if y < 2:\n",
        "    x_test.append(xtest[i])\n",
        "    y_test.append(y)\n",
        "\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "x_test, y_test = np.array(x_test), np.array(y_test)\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (10000, 32, 32, 3) - y_train shape: (10000, 1)\n",
            "x_test shape: (2000, 32, 32, 3) - y_test shape: (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3diIJa0igGQm"
      },
      "source": [
        "# Visualize some images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "IQlS7pMzgFVh",
        "outputId": "1d531908-446b-4a0a-e8ae-5b3d97a00c81"
      },
      "source": [
        "# Plot a few images\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axis = plt.subplots(1, 4, figsize=(10,10))\n",
        "for i, ax in enumerate(axis.flat):\n",
        "    ax.set_title(y_train[i][0])\n",
        "    ax.imshow(x_train[i])\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzYriUPu9jps"
      },
      "source": [
        "# Configure the hyperparameters, 10 epoch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQm4N-Qw9jps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de464b8-ccc3-4d01-b83d-6a6449192c75"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 2  # Size of the patches to be extract from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "latent_dim = 256  # Size of the latent array.\n",
        "projection_dim = 256  # Embedding size of each element in the data and latent arrays.\n",
        "num_heads = 8  # Number of Transformer heads.\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]  # Size of the Transformer Feedforward network.\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2  # Repetitions of the cross-attention and Transformer modules.\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]  # Size of the Feedforward network of the final classifier.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 1024\n",
            "Elements per patch (3 channels): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 1024 X 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fB4YMcP9jpt"
      },
      "source": [
        "Note that, in order to use each pixel as an individual input in the data array,\n",
        "set `patch_size` to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J05zVBqb9jpu"
      },
      "source": [
        "# Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuSJ69D39jpu"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEDYiebxgjzN"
      },
      "source": [
        "# Build the model\n",
        "## 1. Feedforward network (FFN)  \n",
        "## 2. Add patch encoder layer \n",
        "## 3. Add perceiver with cross attention, transformer and perceiver  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z235iGr_9jpv"
      },
      "source": [
        "## Implement Feedforward network (FFN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FadTm9Sr9jpv"
      },
      "source": [
        "# Feedforward network\n",
        "def create_ffn(hidden_units, dropout_rate):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = keras.Sequential(ffn_layers)\n",
        "    return ffn\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5V62NLX9jpw"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO79qxnh9jpw"
      },
      "source": [
        "# Patch encoder layer\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NktJ1kzI9jpw"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into\n",
        "a vector of size `latent_dim`. In addition, it adds a learnable position embedding\n",
        "to the projected vector.\n",
        "\n",
        "Note that the orginal Perceiver paper uses the Fourier feature positional encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3mDwkfG9jpx"
      },
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKF1bacS9jpx"
      },
      "source": [
        "## Build the Perceiver model\n",
        "\n",
        "The Perceiver consists of two modules: a cross-attention\n",
        "module and a standard Transformer with self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIPv5Rfu9jpx"
      },
      "source": [
        "### Cross-attention module\n",
        "\n",
        "The cross-attention expects a `(latent_dim, projection_dim)` latent array,\n",
        "and the `(data_dim,  projection_dim)` data array as inputs,\n",
        "to produce a `(latent_dim, projection_dim)` latent array as an output.\n",
        "To apply cross-attention, the `query` vectors are generated from the latent array,\n",
        "while the `key` and `value` vectors are generated from the encoded image.\n",
        "\n",
        "Note that the data array in this example is the image,\n",
        "where the `data_dim` is set to the `num_patches`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6a6-hVd9jpx"
      },
      "source": [
        "\n",
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n",
        "):\n",
        "\n",
        "    inputs = {\n",
        "        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n",
        "    }\n",
        "\n",
        "    # Apply layer norm to the inputs\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoP8Lhgd9jpy"
      },
      "source": [
        "### Transformer module\n",
        "\n",
        "The Transformer expects the output latent vector from the cross-attention module\n",
        "as an input, applies multi-head self-attention to its `latent_dim` elements,\n",
        "followed by feedforward network, to produce another `(latent_dim, projection_dim)` latent array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__FsVXLr9jpy"
      },
      "source": [
        "\n",
        "def create_transformer_module(\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "):\n",
        "\n",
        "    # input_shape: [1, latent_dim, projection_dim]\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "\n",
        "    x0 = inputs\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Apply layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        # Create a multi-head self-attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        # Apply layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # Apply Feedforward network.\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        # Skip connection 2.\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPie4DBM9jpy"
      },
      "source": [
        "### Perceiver model\n",
        "\n",
        "The Perceiver model repeats the cross-attention and Transformer modules\n",
        "`num_iterations` times—with shared weights and skip connections—to allow\n",
        "the latent array to iteratively extract information from the input image as it is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf6dY2MC9jp0"
      },
      "source": [
        "\n",
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create latent array.\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Create patching module.\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # Create patch encoder.\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "        # Create cross-attenion module.\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create Transformer module.\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create global average pooling layer.\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "        # Create a classification head.\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Augment data.\n",
        "        augmented = data_augmentation(inputs)\n",
        "        # Create patches.\n",
        "        patches = self.patcher(augmented)\n",
        "        # Encode patches.\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        # Prepare cross-attention inputs.\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        # Apply the cross-attention and the Transformer modules iteratively.\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Apply cross-attention from the latent array to the data array.\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            # Apply self-attention Transformer to the latent array.\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            # Set the latent array of the next iteration.\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        # Generate logits.\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMQESw749jp0"
      },
      "source": [
        "# Compile, train, and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llGH48Rn9jp0"
      },
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "\n",
        "    # Create LAMB optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.LAMB(\n",
        "        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=3\n",
        "    )\n",
        "\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY4--mkg9jp1"
      },
      "source": [
        "Note that training the perceiver model with the current settings on a V100 GPUs takes\n",
        "around 200 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-zbM42f9jp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865d923e-41f5-435b-baed-037d8d2a1f3d"
      },
      "source": [
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "141/141 [==============================] - 1523s 11s/step - loss: 0.6261 - acc: 0.6498 - top5-acc: 1.0000 - val_loss: 0.5197 - val_acc: 0.7670 - val_top5-acc: 1.0000\n",
            "Epoch 2/5\n",
            "141/141 [==============================] - 1483s 11s/step - loss: 0.5418 - acc: 0.7236 - top5-acc: 1.0000 - val_loss: 0.4319 - val_acc: 0.8090 - val_top5-acc: 1.0000\n",
            "Epoch 3/5\n",
            "141/141 [==============================] - 1488s 11s/step - loss: 0.4972 - acc: 0.7606 - top5-acc: 1.0000 - val_loss: 0.4047 - val_acc: 0.8240 - val_top5-acc: 1.0000\n",
            "Epoch 4/5\n",
            "141/141 [==============================] - 1488s 11s/step - loss: 0.4605 - acc: 0.7804 - top5-acc: 1.0000 - val_loss: 0.3811 - val_acc: 0.8390 - val_top5-acc: 1.0000\n",
            "Epoch 5/5\n",
            "141/141 [==============================] - 1505s 11s/step - loss: 0.4210 - acc: 0.8070 - top5-acc: 1.0000 - val_loss: 0.3574 - val_acc: 0.8390 - val_top5-acc: 1.0000\n",
            "63/63 [==============================] - 129s 2s/step - loss: 0.3622 - acc: 0.8420 - top5-acc: 1.0000\n",
            "Test accuracy: 84.2%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xxz9s9N9jp1"
      },
      "source": [
        "After 5 epochs, the Perceiver model on the cifar-10 dataset achieved 84.2% accuracy and 100% top-5 accuracy on the test data."
      ]
    }
  ]
}